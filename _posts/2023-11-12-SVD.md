---
layout: post
title:  "SVD and user recommendations"
date:   2023-11-13 11:00:40
blurb: "Using Linear Algebra as a tool for user recommendations"
og_image: /assets/img/content/post-example/Banner.jpg
---

# SVD for user recommendations



SVD in general can be seen as a data reduction tool, which has ties to the fourier transform, which is this beautiful way of approximating functions with the help of sinus and cosinus waves. Further more it has the nice property of always existing and being a unique solution. Before we start digesting how the mathematics are done, let's get a general feel for the Algorithm.

The formula is:

$A = U \Sigma V^T$.

Where A is the matrix that is known beforehand, 

**Eckart-Young Theorem (1936):** For any matrix \(A\) and a given rank \(r\), the best rank-r approximation of \(A\) with respect to the Frobenius norm is achieved by the truncated SVD. In other words, if \(A = ÛΣV^T\) is the SVD of \(A\), then the best rank-r approximation is given by:

$$A_r = UΣ_rV^T,$$

where \(Σ_r\) is obtained from \(Σ\) by retaining the largest \(r\) singular values and setting the others to zero.


For any matrix \(A\), a given rank \(r\), and the Frobenius norm, the best rank-r approximation of \(A\) can be expressed as an argmin problem. Let \(\| \cdot \|_F\) represent the Frobenius norm. Then, the best rank-r approximation of \(A\) can be defined as:

$
A_r = \underset{Â}{\text{argmin}} \left\{ \| A - Â \|_F \; \text{s.t.} \; \text{rank}(A) = r \right\}.
$

In other words, \(A_r\) is the matrix \(B\) that minimizes the Frobenius norm of the difference between \(A\) and \(B\), subject to the constraint that the rank of \(B\) is equal to \(r\).
